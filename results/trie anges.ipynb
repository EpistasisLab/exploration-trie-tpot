{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmarking\n",
      "                                                                          \n",
      "Generation 1 - Current best internal CV score: 0.8160431654676259\n",
      "                                                                          \n",
      "Best pipeline: ExtraTreesClassifier(MultinomialNB(ZeroCount(input_matrix), alpha=10.0, fit_prior=False), bootstrap=True, criterion=gini, max_features=0.15000000000000002, min_samples_leaf=14, min_samples_split=20, n_estimators=100)\n",
      "/baseline/baseline\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to evaluate terminal: AdaBoostClassifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py:140\u001b[0m, in \u001b[0;36mPrimitiveTree.from_string\u001b[0;34m(cls, string, pset)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/matsumoton/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py?line=138'>139</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/matsumoton/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py?line=139'>140</a>\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m(token)\n\u001b[1;32m    <a href='file:///Users/matsumoton/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py?line=140'>141</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AdaBoostClassifier' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie anges.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 238>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=250'>251</a>\u001b[0m     result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39mload()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=251'>252</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k , v \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=252'>253</a>\u001b[0m         \u001b[39m#print(k)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=253'>254</a>\u001b[0m         pipeline_trie\u001b[39m.\u001b[39;49minsert(k,v,tpot\u001b[39m.\u001b[39;49m_pset)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=255'>256</a>\u001b[0m pipeline_trie\u001b[39m.\u001b[39mget_networkx_graph(\u001b[39m100\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=257'>258</a>\u001b[0m \u001b[39m#pipeline_trie.display(f\"/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/anges{directoryev}_run{i}_ds_{pipeline_trie.root.diversity_score}\")\u001b[39;00m\n",
      "\u001b[1;32m/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie anges.ipynb Cell 1'\u001b[0m in \u001b[0;36mPipelineTrie.insert\u001b[0;34m(self, pipeline_str, pipeline_data, pset)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=59'>60</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=60'>61</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=62'>63</a>\u001b[0m pipeline \u001b[39m=\u001b[39m creator\u001b[39m.\u001b[39;49mIndividual\u001b[39m.\u001b[39;49mfrom_string(pipeline_str, pset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=64'>65</a>\u001b[0m \u001b[39m#convert pipeline into a list and change all hyperparameters to None\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/trie%20anges.ipynb#ch0000000?line=65'>66</a>\u001b[0m tree \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py:142\u001b[0m, in \u001b[0;36mPrimitiveTree.from_string\u001b[0;34m(cls, string, pset)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/matsumoton/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py?line=139'>140</a>\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m(token)\n\u001b[1;32m    <a href='file:///Users/matsumoton/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py?line=140'>141</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/matsumoton/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py?line=141'>142</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to evaluate terminal: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(token))\n\u001b[1;32m    <a href='file:///Users/matsumoton/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py?line=143'>144</a>\u001b[0m \u001b[39mif\u001b[39;00m type_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/matsumoton/opt/anaconda3/envs/tpot_env_networkx/lib/python3.9/site-packages/deap/gp.py?line=144'>145</a>\u001b[0m     type_ \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(token)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to evaluate terminal: AdaBoostClassifier."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cmath import inf\n",
    "os.chdir('/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmarking')\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "sys.path.insert(1,'/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmarking')\n",
    "#load pickle evaluated pipelines\n",
    "from os.path import exists\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tpot.tpot import TPOTClassifier\n",
    "import deap\n",
    "from deap import creator\n",
    "\n",
    "import pydot \n",
    "from IPython.display import Image, display\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import math\n",
    "\n",
    "import networkx.algorithms as na\n",
    "\n",
    "class TrieNode:\n",
    " \n",
    "    def __init__(self, primitive):\n",
    "        self.primitive = primitive\n",
    "        self.path = 'root'\n",
    "        self.traverse_count = 0\n",
    "        self.total_cv_score = []\n",
    "        self.generation = []\n",
    "        self.children = {}\n",
    "        self.parents = []\n",
    "        self.depth = 0\n",
    "        self.max_score = -inf\n",
    "        self.min_score = inf\n",
    "        self.diversity_score = 0\n",
    " \n",
    "class PipelineTrie(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        self.root = TrieNode(\"\")\n",
    "        self.graph = {}\n",
    "        \n",
    "    def insert(self, pipeline_str,pipeline_data,pset):\n",
    "\n",
    "        def prim_to_list(prim, args):\n",
    "            if isinstance(prim, deap.gp.Terminal):\n",
    "                return None\n",
    "            return [prim.name] + args\n",
    "        def remove_none(obj):\n",
    "            if isinstance(obj, (list, tuple, set)):\n",
    "                return type(obj)(remove_none(x) for x in obj if x is not None)\n",
    "            elif isinstance(obj, dict):\n",
    "                return type(obj)((remove_none(k), remove_none(v))\n",
    "                for k, v in obj.items() if k is not None and v is not None)\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        pipeline = creator.Individual.from_string(pipeline_str, pset)\n",
    "\n",
    "        #convert pipeline into a list and change all hyperparameters to None\n",
    "        tree = []\n",
    "        stack = []\n",
    "        for node in pipeline:\n",
    "            stack.append((node, []))\n",
    "            while len(stack[-1][1]) == stack[-1][0].arity:\n",
    "                prim, args = stack.pop()\n",
    "                tree = prim_to_list(prim, args)\n",
    "                if len(stack) == 0:\n",
    "                    break  # If stack is empty, all nodes should have been seen\n",
    "                \n",
    "                stack[-1][1].append(tree)\n",
    "        \n",
    "        #remove all Nones\n",
    "        tree = remove_none(tree)\n",
    "        \n",
    "        #dfs through the tree and integrate into trie\n",
    "        stack = []\n",
    "        stack.append(tree)\n",
    "        trie_stack = [self.root]\n",
    "\n",
    "        while stack:\n",
    "            s = stack.pop()\n",
    "            node = trie_stack.pop()\n",
    "            cur_depth = node.depth+1\n",
    "            \n",
    "            if (s[0]) not in node.children:\n",
    "                node.children[(s[0])] = TrieNode(s[0])\n",
    "                node.children[(s[0])].parents = np.append(node.parents,node)\n",
    "                #add a value to the root diversity metric\n",
    "                #self.root.diversity_score =  self.root.diversity_score + 1/cur_depth**2\n",
    "                temp_depth = 1\n",
    "                for tempnode in node.parents:\n",
    "                    tempnode.diversity_score =  tempnode.diversity_score + 1/temp_depth**2\n",
    "                    temp_depth = temp_depth + 1\n",
    "            node.children[(s[0])].traverse_count = node.children[(s[0])].traverse_count + 1\n",
    "            node.children[(s[0])].total_cv_score.append(pipeline_data[\"internal_cv_score\"])\n",
    "            node.children[(s[0])].generation.append(pipeline_data[\"generation\"])\n",
    "            node.children[(s[0])].depth = cur_depth\n",
    "            if not math.isnan(pipeline_data[\"internal_cv_score\"]) and not math.isinf(pipeline_data[\"internal_cv_score\"]):\n",
    "                node.children[(s[0])].min_score = min(node.children[(s[0])].min_score,pipeline_data[\"internal_cv_score\"])\n",
    "                node.children[(s[0])].max_score = max(node.children[(s[0])].max_score,pipeline_data[\"internal_cv_score\"])\n",
    "                self.root.min_score = min(self.root.min_score,pipeline_data[\"internal_cv_score\"])\n",
    "                self.root.max_score = max(self.root.max_score,pipeline_data[\"internal_cv_score\"])\n",
    "            if node.path != 'root':\n",
    "                node.children[(s[0])].path = node.path + '-' + s[0]\n",
    "            else:\n",
    "                node.children[(s[0])].path = s[0]\n",
    "            if len(s[1:]) > 0:\n",
    "                stack.extend(s[1:])\n",
    "                for i in range(len(s[1:])):\n",
    "                    trie_stack.append(node.children[(s[0])])\n",
    "                    \n",
    "    def get_networkx_graph(self,depth=100):\n",
    "        import networkx as nx\n",
    "        from pyvis.network import Network\n",
    "        import matplotlib as mpl\n",
    "\n",
    "        def colorFader(c1,c2,mix=0): #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\n",
    "            c1=np.array(mpl.colors.to_rgb(c1))\n",
    "            c2=np.array(mpl.colors.to_rgb(c2))\n",
    "            return mpl.colors.to_hex((1-mix)*c1 + mix*c2)\n",
    "\n",
    "        c1='red' #blue\n",
    "        c2='green' #green\n",
    "\n",
    "        graph = pydot.Dot(graph_type='graph') \n",
    "        stack = [self.root]\n",
    "        parent_stack = []\n",
    "\n",
    "        max_height = depth\n",
    "        while stack:\n",
    "            s = stack.pop()\n",
    "            if s.depth >= max_height:\n",
    "                continue\n",
    "            for k in s.children.keys():\n",
    "                stack.append(s.children[k])\n",
    "                temp =  [v for v in s.total_cv_score if not math.isnan(v) and not math.isinf(v)]\n",
    "                if len(temp) :\n",
    "                    parentnodeaccuracy =(sum(temp)/len(temp))\n",
    "                    if parentnodeaccuracy > self.root.max_score:\n",
    "                        parentnodeaccuracy = self.root.max_score\n",
    "                    parentnodecolor = colorFader(c1,c2,(parentnodeaccuracy-self.root.min_score)/(self.root.max_score-self.root.min_score))\n",
    "                else:\n",
    "                    parentnodeaccuracy = 'NA'\n",
    "                    parentnodecolor = \"#666666\"\n",
    "                    \n",
    "                temp =  [v for v in s.children[k].total_cv_score if not math.isnan(v) and not math.isinf(v)]\n",
    "                if len(temp) :\n",
    "                    childaccuracy = (sum(temp)/len(temp))\n",
    "                    #floating point 0.00...01 issue\n",
    "                    if childaccuracy > self.root.max_score:\n",
    "                        childaccuracy = self.root.max_score\n",
    "                    childcolor = colorFader(c1,c2,(childaccuracy-self.root.min_score)/(self.root.max_score-self.root.min_score))\n",
    "                    \n",
    "                else:\n",
    "                    childaccuracy = 'NA'\n",
    "                    childcolor = \"#666666\"\n",
    "                \n",
    "                graph.add_node(pydot.Node(s.path,label=s.primitive+'\\n'+str(parentnodeaccuracy),color=parentnodecolor,size=10*(math.tanh(-s.depth+4)+2)))\n",
    "                graph.add_node(pydot.Node(s.children[k].path,label=s.children[k].primitive+'\\n'+str(childaccuracy),color=childcolor,size=10*(math.tanh(-s.children[k].depth+4)+2)))\n",
    "                \n",
    "                edge = pydot.Edge(s.path, s.children[k].path,weight=1,color='#515ba3',value=math.log(s.children[k].traverse_count))\n",
    "                graph.add_edge(edge)\n",
    "        self.graph = nx.nx_pydot.from_pydot(graph)\n",
    "        \n",
    "                \n",
    "    def display(self,filename):\n",
    "        import networkx as nx\n",
    "        from pyvis.network import Network\n",
    "        import matplotlib as mpl\n",
    "        nt = Network(height='100%', width='100%', bgcolor='#333333', font_color='white')\n",
    "        nt.from_nx(self.graph)\n",
    "        nt.show(filename+'.html')\n",
    "        nx.write_edgelist(self.graph, filename+\".edgelist\")\n",
    " \n",
    "    \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from os import makedirs\n",
    "\n",
    "\n",
    "def extract_labels(df, labelname):\n",
    "    y = df[labelname].copy(deep=True)\n",
    "    x = df.drop(labelname, axis=1)\n",
    "    x, y = shuffle(x, y)\n",
    "    x = x.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "#directoryevs = [\"/baseline_dynamic/baseline\",\"/lexicase_dynamic/anges\"]\n",
    "#directoryevs = [\"_baseline\",\"\"]\n",
    "\n",
    "directoryevs = [\"/baseline/baseline\",\"/baseline_dynamic/baseline_dynamic\",\"/lexicase/lexicase\",\"/lexicase_dynamic/lexicase_dynamic\"]\n",
    "directoryevs = [\"/baseline_dynamic/baseline_dynamic\",\"/lexicase/lexicase\",\"/lexicase_dynamic/lexicase_dynamic\"]\n",
    "#directoryevs = [\"/lexicase_dynamic/lexicase_dynamic\"]\n",
    "directoryevs = [\"/baseline/baseline\",\"/lexicase/lexicase\"]\n",
    "upper_quantile_only = False\n",
    "\n",
    "generation_num = 50\n",
    "total_runs = 40\n",
    "\n",
    "name_values = {\n",
    "    \"/baseline/baseline\" : \"baseline\",\n",
    "    \"/baseline_dynamic/baseline_dynamic\" : \"baseline_dynamic\",\n",
    "    \"/lexicase/lexicase\" : \"lexicase\",\n",
    "    \"/lexicase_dynamic/lexicase_dynamic\" : \"lexicase_dynamic\"\n",
    "}\n",
    "\n",
    "#directoryevs = [\"lexicase_dynamic_final\"]\n",
    "result = {}\n",
    "\n",
    "import pandas as pd\n",
    " # This is done based on the dataset ID.\n",
    "#dataset = openml.datasets.get_dataset(1164)\n",
    "#dataset = openml.datasets.get_dataset(1164)\n",
    "dataset = pd.read_csv(\"/Users/matsumoton/Documents/anges_cad_1_train.csv\",sep=\",\")\n",
    "y_train = dataset['target']\n",
    "X_train = dataset.drop(['target'],axis=1)\n",
    "\n",
    "test_dataset = pd.read_csv(\"/Users/matsumoton/Documents/anges_cad_1_test.csv\",sep=\",\")\n",
    "y_test = test_dataset['target']\n",
    "X_test = test_dataset.drop(['target'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "tpot = TPOTClassifier(verbosity=2, population_size=1, generations=1)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "diversity_scores= {}\n",
    "\n",
    "for directoryev in directoryevs:\n",
    "    temp_ev = []\n",
    "    #pipeline_trie = PipelineTrie()\n",
    "    diversity_scores[directoryev] = []\n",
    "    print(directoryev)\n",
    "    for i in range(0,total_runs):\n",
    "        pipeline_trie = PipelineTrie()\n",
    "        pklfile = f\"/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/anges{directoryev}_{i}_evaluated_individuals.pkl\"\n",
    "        if not exists(pklfile):\n",
    "                continue\n",
    "        with open(pklfile, 'rb') as file:\n",
    "        #with open(f\"C:/Users/matsumoton/Box/tpot_benchmark_data/results_pop40_gen20_{directoryev}/pipelines/digen{j}_run_{i}_evaluated_individuals.pkl\", 'rb') as file:\n",
    "            unpickler = pickle.Unpickler(file)\n",
    "            result = unpickler.load()\n",
    "            for k , v in result.items():\n",
    "                #print(k)\n",
    "                pipeline_trie.insert(k,v,tpot._pset)\n",
    "        \n",
    "        pipeline_trie.get_networkx_graph(100)\n",
    "\n",
    "        #pipeline_trie.display(f\"/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/anges{directoryev}_run{i}_ds_{pipeline_trie.root.diversity_score}\")\n",
    "        print(\"global efficiency run \" + str(i) + \" : \" + str(na.global_efficiency(pipeline_trie.graph)))\n",
    "        A = nx.adjacency_matrix(pipeline_trie.graph)\n",
    "\n",
    "        A.todense()\n",
    "\n",
    "        exit\n",
    "        #if i == 5:\n",
    "        #    break\n",
    "\n",
    "        \n",
    "        #print(directoryev+' '+str(i) + ' ' + str([pipeline_trie.root.diversity_score,pipeline_trie.root.max_score]))\n",
    "        #diversity_scores[directoryev].append([pipeline_trie.root.diversity_score,pipeline_trie.root.max_score])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2831fd333361432c94fa5bcfb075efcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7816238437821171\n",
      "\n",
      "Best pipeline: LogisticRegression(input_matrix, C=0.001, dual=False, penalty=l2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "tpot = TPOTClassifier(verbosity=2, population_size=1, generations=1)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "for directoryev in directoryevs:\n",
    "    temp_ev = []\n",
    "    for i in range(total_runs):\n",
    "\n",
    "        pipeline_trie = PipelineTrie()\n",
    "        \n",
    "        #with open(f\"C:/Users/matsumoton/Box/tpot_benchmark_data/results_pop40_gen20_{directoryev}/pipelines/digen{j}_run_{i}_evaluated_individuals.pkl\", 'rb') as file:\n",
    "        with open(f\"/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/anges{directoryev}_{i}_evaluated_individuals.pkl\", 'rb') as file:\n",
    "            unpickler = pickle.Unpickler(file)\n",
    "            result = unpickler.load()\n",
    "            for k , v in result.items():\n",
    "                #print(k)\n",
    "                pipeline_trie.insert(k,v,tpot._pset)\n",
    "\n",
    "        pipeline_trie.display()\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['GaussianNB1', 'RandomForestClassifier1', 'ExtraTreesClassifier1', 'LinearSVC1', 'DecisionTreeClassifier1', 'BernoulliNB1', 'XGBClassifier1', 'MLPClassifier1', 'LogisticRegression1', 'KNeighborsClassifier1', 'SGDClassifier1', 'GradientBoostingClassifier1'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_trie.root.children.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoepslexicase_16_fitness\n",
      "autoepslexicase_33_resources.csv_memory\n",
      "autoepslexicase_1_fitness\n",
      "autoepslexicase_16_mutation_rates\n",
      "autoepslexicase_25_pareto_fitness\n",
      "autoepslexicase_6_evaluated_individuals\n",
      "autoepslexicase_24_fitness\n",
      "autoepslexicase_36_resources.csv_time\n",
      "autoepslexicase_6_mutation_rates\n",
      "autoepslexicase_35_pareto_fitness\n",
      "autoepslexicase_8_fitness\n",
      "autoepslexicase_5_pareto_fitness\n",
      "autoepslexicase_8_pareto_fitness\n",
      "autoepslexicase_36_mutation_rates\n",
      "autoepslexicase_31_fitness\n",
      "autoepslexicase_32_evaluated_individuals\n",
      "autoepslexicase_15_pareto_fitness\n",
      "autoepslexicase_31_resources.csv_time\n",
      "autoepslexicase_14_evaluated_individuals\n",
      "autoepslexicase_25_mutation_rates\n",
      "autoepslexicase_16_pareto_fitness\n",
      "autoepslexicase_34_fitness\n",
      "autoepslexicase_13_evaluated_individuals\n",
      "autoepslexicase_16_resources.csv_memory\n",
      "autoepslexicase_21_resources.csv_memory\n",
      "autoepslexicase_35_mutation_rates\n",
      "autoepslexicase_6_pareto_fitness\n",
      "autoepslexicase_35_evaluated_individuals\n",
      "autoepslexicase_32_resources.csv_time\n",
      "autoepslexicase_25_resources.csv_memory\n",
      "autoepslexicase_12_resources.csv_memory\n",
      "autoepslexicase_1_resources.csv_memory\n",
      "autoepslexicase_20_evaluated_individuals\n",
      "autoepslexicase_4_fitness\n",
      "autoepslexicase_36_pareto_fitness\n",
      "autoepslexicase_8_mutation_rates\n",
      "autoepslexicase_13_fitness\n",
      "autoepslexicase_5_mutation_rates\n",
      "autoepslexicase_5_resources.csv_memory\n",
      "autoepslexicase_1_evaluated_individuals\n",
      "autoepslexicase_35_resources.csv_time\n",
      "autoepslexicase_21_fitness\n",
      "autoepslexicase_15_mutation_rates\n",
      "autoepslexicase_24_resources.csv_memory\n",
      "autoepslexicase_30_pareto_fitness\n",
      "autoepslexicase_13_resources.csv_memory\n",
      "autoepslexicase_22_fitness\n",
      "autoepslexicase_3_mutation_rates\n",
      "autoepslexicase_34_resources.csv_time\n",
      "autoepslexicase_4_evaluated_individuals\n",
      "autoepslexicase_17_resources.csv_memory\n",
      "autoepslexicase_10_fitness\n",
      "autoepslexicase_20_resources.csv_memory\n",
      "autoepslexicase_7_fitness\n",
      "autoepslexicase_13_mutation_rates\n",
      "autoepslexicase_20_pareto_fitness\n",
      "autoepslexicase_8_resources.csv_memory\n",
      "autoepslexicase_25_evaluated_individuals\n",
      "autoepslexicase_33_resources.csv_time\n",
      "autoepslexicase_4_resources.csv_memory\n",
      "autoepslexicase_10_pareto_fitness\n",
      "autoepslexicase_30_evaluated_individuals\n",
      "autoepslexicase_23_mutation_rates\n",
      "autoepslexicase_16_evaluated_individuals\n",
      "autoepslexicase_33_mutation_rates\n",
      "autoepslexicase_0_pareto_fitness\n",
      "autoepslexicase_0_resources.csv_memory\n",
      "autoepslexicase_3_pareto_fitness\n",
      "autoepslexicase_36_resources.csv_memory\n",
      "autoepslexicase_30_resources.csv_time\n",
      "autoepslexicase_30_mutation_rates\n",
      "autoepslexicase_11_evaluated_individuals\n",
      "autoepslexicase_32_fitness\n",
      "autoepslexicase_32_resources.csv_memory\n",
      "autoepslexicase_20_mutation_rates\n",
      "autoepslexicase_13_pareto_fitness\n",
      "autoepslexicase_23_pareto_fitness\n",
      "autoepslexicase_10_mutation_rates\n",
      "autoepslexicase_22_evaluated_individuals\n",
      "autoepslexicase_0_mutation_rates\n",
      "autoepslexicase_3_evaluated_individuals\n",
      "autoepslexicase_33_pareto_fitness\n",
      "autoepslexicase_2_fitness\n",
      "autoepslexicase_15_fitness\n",
      "autoepslexicase_2_evaluated_individuals\n",
      "autoepslexicase_25_resources.csv_time\n",
      "autoepslexicase_2_resources.csv_memory\n",
      "autoepslexicase_7_resources.csv_time\n",
      "autoepslexicase_12_pareto_fitness\n",
      "autoepslexicase_21_mutation_rates\n",
      "autoepslexicase_20_fitness\n",
      "autoepslexicase_23_evaluated_individuals\n",
      "autoepslexicase_6_resources.csv_memory\n",
      "autoepslexicase_2_pareto_fitness\n",
      "autoepslexicase_14_resources.csv_time\n",
      "autoepslexicase_12_fitness\n",
      "autoepslexicase_31_mutation_rates\n",
      "autoepslexicase_5_fitness\n",
      "autoepslexicase_22_resources.csv_time\n",
      "autoepslexicase_1_mutation_rates\n",
      "autoepslexicase_36_evaluated_individuals\n",
      "autoepslexicase_32_pareto_fitness\n",
      "autoepslexicase_0_resources.csv_time\n",
      "autoepslexicase_15_resources.csv_memory\n",
      "autoepslexicase_22_resources.csv_memory\n",
      "autoepslexicase_35_fitness\n",
      "autoepslexicase_11_mutation_rates\n",
      "autoepslexicase_22_pareto_fitness\n",
      "autoepslexicase_11_resources.csv_memory\n",
      "autoepslexicase_13_resources.csv_time\n",
      "autoepslexicase_10_evaluated_individuals\n",
      "autoepslexicase_17_evaluated_individuals\n",
      "autoepslexicase_21_pareto_fitness\n",
      "autoepslexicase_12_mutation_rates\n",
      "autoepslexicase_3_resources.csv_time\n",
      "autoepslexicase_21_resources.csv_time\n",
      "autoepslexicase_31_pareto_fitness\n",
      "autoepslexicase_10_resources.csv_time\n",
      "autoepslexicase_2_mutation_rates\n",
      "autoepslexicase_31_evaluated_individuals\n",
      "autoepslexicase_30_fitness\n",
      "autoepslexicase_30_resources.csv_memory\n",
      "autoepslexicase_25_fitness\n",
      "autoepslexicase_32_mutation_rates\n",
      "autoepslexicase_4_resources.csv_time\n",
      "autoepslexicase_24_evaluated_individuals\n",
      "autoepslexicase_1_pareto_fitness\n",
      "autoepslexicase_17_resources.csv_time\n",
      "autoepslexicase_34_resources.csv_memory\n",
      "autoepslexicase_0_fitness\n",
      "autoepslexicase_17_fitness\n",
      "autoepslexicase_22_mutation_rates\n",
      "autoepslexicase_11_pareto_fitness\n",
      "autoepslexicase_5_evaluated_individuals\n",
      "autoepslexicase_34_mutation_rates\n",
      "autoepslexicase_0_evaluated_individuals\n",
      "autoepslexicase_7_pareto_fitness\n",
      "autoepslexicase_14_fitness\n",
      "autoepslexicase_3_fitness\n",
      "autoepslexicase_16_resources.csv_time\n",
      "autoepslexicase_5_resources.csv_time\n",
      "autoepslexicase_8_evaluated_individuals\n",
      "autoepslexicase_21_evaluated_individuals\n",
      "autoepslexicase_17_pareto_fitness\n",
      "autoepslexicase_24_mutation_rates\n",
      "autoepslexicase_33_fitness\n",
      "autoepslexicase_34_evaluated_individuals\n",
      "autoepslexicase_11_resources.csv_time\n",
      "autoepslexicase_14_mutation_rates\n",
      "autoepslexicase_35_resources.csv_memory\n",
      "autoepslexicase_2_resources.csv_time\n",
      "autoepslexicase_20_resources.csv_time\n",
      "autoepslexicase_12_evaluated_individuals\n",
      "autoepslexicase_8_resources.csv_time\n",
      "autoepslexicase_31_resources.csv_memory\n",
      "autoepslexicase_4_mutation_rates\n",
      "autoepslexicase_15_evaluated_individuals\n",
      "autoepslexicase_12_resources.csv_time\n",
      "autoepslexicase_7_resources.csv_memory\n",
      "autoepslexicase_36_fitness\n",
      "autoepslexicase_7_mutation_rates\n",
      "autoepslexicase_34_pareto_fitness\n",
      "autoepslexicase_24_pareto_fitness\n",
      "autoepslexicase_33_evaluated_individuals\n",
      "autoepslexicase_17_mutation_rates\n",
      "autoepslexicase_3_resources.csv_memory\n",
      "autoepslexicase_23_resources.csv_time\n",
      "autoepslexicase_1_resources.csv_time\n",
      "autoepslexicase_14_pareto_fitness\n",
      "autoepslexicase_6_fitness\n",
      "autoepslexicase_11_fitness\n",
      "autoepslexicase_15_resources.csv_time\n",
      "autoepslexicase_10_resources.csv_memory\n",
      "autoepslexicase_4_pareto_fitness\n",
      "autoepslexicase_23_fitness\n",
      "autoepslexicase_14_resources.csv_memory\n",
      "autoepslexicase_23_resources.csv_memory\n",
      "autoepslexicase_7_evaluated_individuals\n",
      "autoepslexicase_24_resources.csv_time\n",
      "autoepslexicase_6_resources.csv_time\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for count, f in enumerate(os.listdir(\"/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/anges/autoepslexicase\")):\n",
    "    f_name, f_ext = os.path.splitext(f)\n",
    "    #print(f_name)\n",
    "    f_name = f_name.split(\"_\")[2:]\n",
    "    f_name = \"autoepslexicase_\"+\"_\".join(f_name)\n",
    "    print(f_name)\n",
    "    new_name = f'/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/anges/autoepslexicase/{f_name}{f_ext}'\n",
    "    #os.rename(f'/Users/matsumoton/Library/CloudStorage/Box-Box/tpot_benchmark_data/anges/autoepslexicase/{f}', new_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ba7a57e6e00c9e1c2f2b594a0a80dbdb756f65337f87032505bbd79dbbe7d2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tpot_env_networkx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
